{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c846cc-7e2e-4dfb-90cd-d4cef7e646a6",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788d5c92-ba7a-4fc0-8dde-c6a763e4fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import gzip\n",
    "import tempfile\n",
    "from collections import defaultdict, Counter\n",
    "from intervaltree import IntervalTree\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import time\n",
    "import subprocess\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7235348-ace5-4593-8612-8fc967c0d99a",
   "metadata": {},
   "source": [
    "## GTF Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22560238-974f-4676-8102-f84c17e47823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gtf_attributes(attr_string):\n",
    "    attrs = {}\n",
    "    for attr in attr_string.strip(';').split(';'):\n",
    "        attr = attr.strip()\n",
    "        if not attr:\n",
    "            continue\n",
    "        parts = attr.split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            attrs[parts[0]] = parts[1].strip('\"')\n",
    "    return attrs\n",
    "\n",
    "def determine_utr_type(exon_number):\n",
    "    exon_str = str(exon_number).strip()\n",
    "    if exon_str == '1':\n",
    "        return \"5' UTR\"\n",
    "    return \"3' UTR\"\n",
    "\n",
    "def build_interval_trees_fast(gtf_file):\n",
    "    trees = defaultdict(IntervalTree)\n",
    "    feature_counts = Counter()\n",
    "    \n",
    "    valid_features = {'exon', 'CDS', 'UTR', 'start_codon', 'stop_codon'}\n",
    "    \n",
    "    print(f\"Building interval trees from {gtf_file}...\")\n",
    "    \n",
    "    with open(gtf_file, 'r') as f:\n",
    "        for line in tqdm(f, desc=\"Loading GTF\", unit=\"lines\"):\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            fields = line.split('\\t', 9)\n",
    "            if len(fields) < 9:\n",
    "                continue\n",
    "            \n",
    "            feature_type = fields[2]\n",
    "            if feature_type not in valid_features:\n",
    "                continue\n",
    "            \n",
    "            start = int(fields[3]) - 1\n",
    "            end = int(fields[4])\n",
    "            \n",
    "            attrs = {}\n",
    "            attr_string = fields[8]\n",
    "            \n",
    "            if 'gene_id \"' in attr_string:\n",
    "                gene_start = attr_string.find('gene_id \"') + 9\n",
    "                gene_end = attr_string.find('\"', gene_start)\n",
    "                attrs['gene_id'] = attr_string[gene_start:gene_end]\n",
    "            \n",
    "            if 'gene_name \"' in attr_string:\n",
    "                name_start = attr_string.find('gene_name \"') + 11\n",
    "                name_end = attr_string.find('\"', name_start)\n",
    "                attrs['gene_name'] = attr_string[name_start:name_end]\n",
    "            \n",
    "            if feature_type == 'UTR' and 'exon_number ' in attr_string:\n",
    "                exon_start = attr_string.find('exon_number ') + 12\n",
    "                exon_end = attr_string.find(';', exon_start)\n",
    "                if exon_end == -1:\n",
    "                    exon_end = len(attr_string)\n",
    "                exon_number = attr_string[exon_start:exon_end].strip().strip('\"')\n",
    "                display_feature = determine_utr_type(exon_number)\n",
    "            else:\n",
    "                display_feature = feature_type\n",
    "            \n",
    "            data = {\n",
    "                'feature': display_feature,\n",
    "                'gene_id': attrs.get('gene_id', ''),\n",
    "                'gene_name': attrs.get('gene_name', ''),\n",
    "                'strand': fields[6]\n",
    "            }\n",
    "            \n",
    "            trees[fields[0]][start:end] = data\n",
    "            feature_counts[display_feature] += 1\n",
    "    \n",
    "    print(f\"Loaded {sum(feature_counts.values())} features\")\n",
    "    \n",
    "    return dict(trees), dict(feature_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e679312-56ca-4062-8d81-7e57aabc6551",
   "metadata": {},
   "source": [
    "## Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e783482d-f19a-46db-acde-250a824ec8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bed_chunk_to_file(args):\n",
    "    chunk_data, trees, chunk_id, temp_dir = args\n",
    "    \n",
    "    temp_file = os.path.join(temp_dir, f\"chunk_{chunk_id:05d}.tmp\")\n",
    "    stats = Counter()\n",
    "    seen_positions = set()\n",
    "    \n",
    "    with open(temp_file, 'w') as outf:\n",
    "        for line in chunk_data:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            fields = line.strip().split('\\t')\n",
    "            \n",
    "            if len(fields) < 6:\n",
    "                stats['skipped'] += 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                chrom = fields[0]\n",
    "                start = int(fields[1])\n",
    "                end = int(fields[2])\n",
    "                modification = fields[3]\n",
    "                score = fields[4]\n",
    "                strand = fields[5]\n",
    "                \n",
    "                pos_key = (chrom, start, strand, modification)\n",
    "                if pos_key in seen_positions:\n",
    "                    stats['duplicates'] += 1\n",
    "                    continue\n",
    "                \n",
    "                seen_positions.add(pos_key)\n",
    "                \n",
    "                if chrom in trees:\n",
    "                    overlaps = trees[chrom][start]\n",
    "                    \n",
    "                    if overlaps:\n",
    "                        for interval in overlaps:\n",
    "                            data = interval.data\n",
    "                            if data['strand'] == strand:\n",
    "                                stats['annotated'] += 1\n",
    "                                \n",
    "                                result = [\n",
    "                                    chrom, start, end, modification,\n",
    "                                    interval.begin, interval.end,\n",
    "                                    score, strand,\n",
    "                                    data['gene_id'],\n",
    "                                    data['gene_name'],\n",
    "                                    data['feature'],\n",
    "                                    fields[10] if len(fields) > 10 else '.',\n",
    "                                    fields[11] if len(fields) > 11 else '.',\n",
    "                                    fields[12] if len(fields) > 12 else '.',\n",
    "                                    fields[18] if len(fields) > 18 else '.'\n",
    "                                ]\n",
    "                                \n",
    "                                outf.write('\\t'.join(str(x) for x in result) + '\\n')\n",
    "                                stats[f\"feature_{data['feature']}\"] += 1\n",
    "                    else:\n",
    "                        stats['no_overlap'] += 1\n",
    "                else:\n",
    "                    stats['no_overlap'] += 1\n",
    "                    \n",
    "            except (ValueError, IndexError) as e:\n",
    "                stats['errors'] += 1\n",
    "                continue\n",
    "    \n",
    "    return temp_file, stats\n",
    "\n",
    "def annotate_bed_file(bed_file, trees, output_file, n_processes=None):\n",
    "    if n_processes is None:\n",
    "        n_processes = min(cpu_count(), 20)\n",
    "    \n",
    "    print(f\"\\nAnnotating {bed_file} using {n_processes} processes...\")\n",
    "    \n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"gencode_annotator_\")\n",
    "    \n",
    "    try:\n",
    "        with open(bed_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        print(f\"Total lines: {len(lines):,}\")\n",
    "        \n",
    "        chunk_size = max(100000, len(lines) // (n_processes * 4))\n",
    "        chunk_size = min(chunk_size, 5000000)\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(0, len(lines), chunk_size):\n",
    "            chunk = lines[i:i + chunk_size]\n",
    "            chunks.append((chunk, trees, len(chunks), temp_dir))\n",
    "        \n",
    "        print(f\"Split into {len(chunks)} chunks\")\n",
    "        \n",
    "        total_stats = Counter()\n",
    "        temp_files = []\n",
    "        \n",
    "        with Pool(n_processes) as pool:\n",
    "            for temp_file, stats in tqdm(\n",
    "                pool.imap_unordered(process_bed_chunk_to_file, chunks),\n",
    "                total=len(chunks),\n",
    "                desc=\"Processing chunks\"\n",
    "            ):\n",
    "                temp_files.append(temp_file)\n",
    "                total_stats.update(stats)\n",
    "        \n",
    "        temp_files.sort()\n",
    "        \n",
    "        print(f\"Merging temporary files...\")\n",
    "        total_written = 0\n",
    "        \n",
    "        with open(output_file, 'w') as outf:\n",
    "            for temp_file in temp_files:\n",
    "                with open(temp_file, 'r') as inf:\n",
    "                    for line in inf:\n",
    "                        outf.write(line)\n",
    "                        total_written += 1\n",
    "                \n",
    "                os.remove(temp_file)\n",
    "        \n",
    "        print(f\"  Annotated positions: {total_stats['annotated']:,}\")\n",
    "        \n",
    "    finally:\n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "    \n",
    "    return total_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbea4e7-02f4-4ddf-9db9-57773b5575da",
   "metadata": {},
   "source": [
    "## Kmer Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a419234-f91e-40ec-b195-b34b0016edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_5mer_surround_pysam(fasta, reference, position):\n",
    "    if position < 3:\n",
    "        return \"N/A\"\n",
    "    \n",
    "    center = position\n",
    "    five_mer_start = center - 2\n",
    "    five_mer_end = center + 3\n",
    "    \n",
    "    try:\n",
    "        five_mer = fasta.fetch(reference, five_mer_start, five_mer_end)\n",
    "        return five_mer\n",
    "    except (KeyError, ValueError):\n",
    "        return \"N/A\"\n",
    "\n",
    "def reverse_complement(seq):\n",
    "    complement = str.maketrans(\"ACGT\", \"TGCA\")\n",
    "    return seq.translate(complement)[::-1]\n",
    "\n",
    "def process_kmer_chunk(chunk_data):\n",
    "    chunk_df, pos_column, fasta_path = chunk_data\n",
    "    chunk_df = chunk_df.copy()\n",
    "    \n",
    "    with pysam.FastaFile(fasta_path) as fasta:\n",
    "        kmers = []\n",
    "        for _, row in chunk_df.iterrows():\n",
    "            kmer = find_5mer_surround_pysam(fasta, row[\"chrom\"], row[pos_column])\n",
    "            kmers.append(kmer)\n",
    "        chunk_df['kmer'] = kmers\n",
    "    \n",
    "    chunk_df = chunk_df[chunk_df['kmer'] != \"N/A\"]\n",
    "    \n",
    "    adjusted_kmers = []\n",
    "    for _, row in chunk_df.iterrows():\n",
    "        if row['strand'] == '+':\n",
    "            adjusted_kmers.append(row['kmer'])\n",
    "        elif row['strand'] == '-':\n",
    "            adjusted_kmers.append(reverse_complement(row['kmer']))\n",
    "        else:\n",
    "            adjusted_kmers.append(\"N/A\")\n",
    "    \n",
    "    chunk_df['adjusted_kmer'] = adjusted_kmers\n",
    "    \n",
    "    return chunk_df\n",
    "\n",
    "def add_5mer_to_df_parallel(df, fasta_path, pos_column=\"drs_start\", n_cores=8):\n",
    "    n_cores = min(n_cores, 32)\n",
    "    \n",
    "    print(f\"Processing k-mers for {len(df):,} rows using {n_cores} cores...\")\n",
    "    \n",
    "    if len(df) > 10_000_000:\n",
    "        chunk_size = 50000\n",
    "    elif len(df) > 1_000_000:\n",
    "        chunk_size = 20000\n",
    "    else:\n",
    "        chunk_size = max(1000, len(df) // (n_cores * 4))\n",
    "    \n",
    "    chunks = [(df.iloc[i:i+chunk_size], pos_column, fasta_path) \n",
    "              for i in range(0, len(df), chunk_size)]\n",
    "    \n",
    "    try:\n",
    "        with Pool(n_cores) as pool:\n",
    "            results = list(tqdm(\n",
    "                pool.imap(process_kmer_chunk, chunks),\n",
    "                total=len(chunks),\n",
    "                desc=\"Processing k-mers\",\n",
    "                unit=\"chunk\"\n",
    "            ))\n",
    "    except Exception as e:\n",
    "        print(f\"Parallel processing error: {e}\")\n",
    "        print(\"Falling back to sequential processing...\")\n",
    "        results = []\n",
    "        for chunk in tqdm(chunks, desc=\"Processing k-mers\", unit=\"chunk\"):\n",
    "            results.append(process_kmer_chunk(chunk))\n",
    "    \n",
    "    result_df = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef8b564-c2dd-4579-9295-839eb6aeb72c",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84ce5c2-f41f-49aa-bebc-b9bfeeb0cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_annotated(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    priority_map = {\n",
    "        'three_prime_UTR': 1, '3\\' UTR': 1,\n",
    "        'five_prime_UTR': 2, '5\\' UTR': 2,\n",
    "        'CDS': 3, 'stop_codon': 4,\n",
    "        'start_codon': 5, 'exon': 999\n",
    "    }\n",
    "    \n",
    "    df['priority'] = df['feature_type'].map(lambda x: priority_map.get(x, 50))\n",
    "    df_sorted = df.sort_values(\n",
    "        by=['chrom', 'drs_start', 'strand', 'mod', 'gene_id', 'priority']\n",
    "    )\n",
    "    df_dedup = df_sorted.drop_duplicates(\n",
    "        subset=['chrom', 'drs_start', 'strand', 'mod', 'gene_id'],\n",
    "        keep='first'\n",
    "    )\n",
    "    df_dedup = df_dedup.drop('priority', axis=1)\n",
    "    \n",
    "    return df_dedup\n",
    "\n",
    "def process_annotated_file(annotated_file, modkit_file, fasta_path, output_name, \n",
    "                          output_dir, n_cores=8):\n",
    "    print(f\"\\nProcessing {output_name}...\")\n",
    "    \n",
    "    # Load modkit data\n",
    "    modkit_df = pd.read_csv(modkit_file, sep='\\t', header=None)\n",
    "    modkit_df.rename(columns={\n",
    "        0: 'chrom', 1: 'start', 2: 'end', 3: 'mod', 4: 'score',\n",
    "        5: 'strand', 6: 'start_1', 7: 'end_1', 8: 'color',\n",
    "        9: 'n_valid_cov', 10: 'mod_percent', 11: 'n_mod',\n",
    "        12: 'n_canon', 13: 'n_other_mod', 14: 'n_delete',\n",
    "        15: 'n_fail', 16: 'n_diff', 17: 'n_no_call',\n",
    "        18: 'fp_adjusted_mod_percent'\n",
    "    }, inplace=True)\n",
    "    modkit_df['fp_adjusted_mod_percent'] = modkit_df['fp_adjusted_mod_percent'].clip(lower=0)\n",
    "    \n",
    "    # Load annotated data\n",
    "    annotated_df = pd.read_csv(annotated_file, sep='\\t', header=None)\n",
    "    annotated_columns = [\n",
    "        'chrom', 'drs_start', 'drs_end', 'mod', 'feature_start', 'feature_end',\n",
    "        'score', 'strand', 'gene_id', 'gene_name', 'feature_type',\n",
    "        'mod_percent', 'n_mod', 'n_canon', 'fp_adjusted_mod_percent'\n",
    "    ]\n",
    "    annotated_df.columns = annotated_columns\n",
    "    \n",
    "    for col in ['score', 'mod_percent', 'n_mod', 'n_canon', 'fp_adjusted_mod_percent']:\n",
    "        annotated_df[col] = pd.to_numeric(annotated_df[col], errors='coerce')\n",
    "    \n",
    "    # Apply quality filtering\n",
    "    print(f\"  Before filtering: {len(annotated_df):,} rows\")\n",
    "    annotated_valid = annotated_df[\n",
    "        (annotated_df['score'] >= 20) & \n",
    "        (annotated_df['fp_adjusted_mod_percent'] >= 20)\n",
    "    ].copy()\n",
    "    print(f\"  After filtering: {len(annotated_valid):,} rows\")\n",
    "    \n",
    "    # Deduplicate\n",
    "    annotated_valid = deduplicate_annotated(annotated_valid)\n",
    "    print(f\"  After deduplication: {len(annotated_valid):,} rows\")\n",
    "    \n",
    "    # Add k-mers\n",
    "    annotated_valid_kmer = add_5mer_to_df_parallel(\n",
    "        annotated_valid, fasta_path, pos_column=\"drs_start\", n_cores=n_cores\n",
    "    )\n",
    "    print(f\"  Final dataset: {len(annotated_valid_kmer):,} rows\")\n",
    "    \n",
    "    # Save\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, f\"{output_name}_annotated_valid_kmer.pkl\")\n",
    "    annotated_valid_kmer.to_pickle(output_file)\n",
    "    print(f\"  Saved to: {output_file}\")\n",
    "    \n",
    "    return annotated_valid_kmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a87cdc-7cfe-4e82-90a7-683a7f95ed44",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9895ee-f953-4e87-b5a7-34fb1759f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download reference files quietly if they don't exist\n",
    "GTF_FILE_GZ = './gencode.v47.annotation.gtf.gz'\n",
    "FASTA_FILE_GZ = './GCA_000001405.15_GRCh38_full_analysis_set.fna.gz'\n",
    "\n",
    "if not os.path.exists(GTF_FILE_GZ):\n",
    "    print(\"Downloading GTF file...\")\n",
    "    subprocess.run(['wget', '-q', '-O', GTF_FILE_GZ, \n",
    "                    'https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_47/gencode.v47.annotation.gtf.gz'])\n",
    "    print(f\"  Downloaded to {GTF_FILE_GZ}\")\n",
    "\n",
    "if not os.path.exists(FASTA_FILE_GZ):\n",
    "    print(\"Downloading FASTA file...\")\n",
    "    subprocess.run(['wget', '-q', '-O', FASTA_FILE_GZ,\n",
    "                    'https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_full_analysis_set.fna.gz'])\n",
    "    print(f\"  Downloaded to {FASTA_FILE_GZ}\")\n",
    "\n",
    "# Decompress files\n",
    "GTF_FILE = GTF_FILE_GZ[:-3]  # Remove .gz extension\n",
    "FASTA_FILE = FASTA_FILE_GZ[:-3]\n",
    "\n",
    "if not os.path.exists(GTF_FILE):\n",
    "    print(\"Decompressing GTF file...\")\n",
    "    subprocess.run(['gunzip', '-k', GTF_FILE_GZ])  # -k keeps the original\n",
    "    print(f\"  Decompressed to {GTF_FILE}\")\n",
    "\n",
    "if not os.path.exists(FASTA_FILE):\n",
    "    print(\"Decompressing FASTA file...\")\n",
    "    subprocess.run(['gunzip', '-k', FASTA_FILE_GZ])\n",
    "    print(f\"  Decompressed to {FASTA_FILE}\")\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = '../../Exemplar_Data/annotated_output/'\n",
    "PICKLE_DIR = '../../Exemplar_Data/annotated_output/pickle_output/'\n",
    "\n",
    "# Input files - These are the modkit output BED files\n",
    "MODKIT_FILES = ['/scratch/stein.an/GM12878_Official/08_07_24_R9RNA_GM12878_mRNA_RT_sup_8mods_polyA_sorted_filtered.chr12-112000000-114000000_pileup_fp_adjusted.tsv']  # Modkit output BED files\n",
    "SAMPLE_NAMES = ['08_07_24_GM12878_chr12-112000000-114000000']  # Names for output files\n",
    "\n",
    "# Processing parameters\n",
    "N_PROCESSES = 8  # For annotation\n",
    "N_CORES = 8       # For k-mer extraction\n",
    "\n",
    "# Cache file for GTF trees (speeds up repeated runs)\n",
    "CACHE_FILE = './gencode.v47.annotation.tree.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035af8a-76f9-488c-b8f7-859df65f1640",
   "metadata": {},
   "source": [
    "## Build / Load Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54c112ca-3b53-4bad-8e45-41e480c3db9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached trees from ./gencode.v47.annotation.tree.pkl\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(CACHE_FILE):\n",
    "    print(f\"Loading cached trees from {CACHE_FILE}\")\n",
    "    with open(CACHE_FILE, 'rb') as f:\n",
    "        trees, feature_counts = pickle.load(f)\n",
    "else:\n",
    "    trees, feature_counts = build_interval_trees_fast(GTF_FILE)\n",
    "    \n",
    "    print(f\"Caching trees to {CACHE_FILE}\")\n",
    "    with open(CACHE_FILE, 'wb') as f:\n",
    "        pickle.dump((trees, feature_counts), f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4e689-ac93-43f6-a4da-b77434183f39",
   "metadata": {},
   "source": [
    "## Process All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d5e8530-1ea5-4df1-96d5-22ef5042f1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RNA Modification Annotation and Processing Pipeline\n",
      "Started at: 2025-11-04 15:36:00\n",
      "============================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Processing sample: 08_07_24_GM12878_chr12-112000000-114000000\n",
      "==================================================\n",
      "\n",
      "Step 1: Annotating modkit BED file with gene information...\n",
      "\n",
      "Annotating /scratch/stein.an/GM12878_Official/08_07_24_R9RNA_GM12878_mRNA_RT_sup_8mods_polyA_sorted_filtered.chr12-112000000-114000000_pileup_fp_adjusted.tsv using 8 processes...\n",
      "Total lines: 670,373\n",
      "Split into 7 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 7/7 [02:23<00:00, 20.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging temporary files...\n",
      "  Annotated positions: 1,088,323\n",
      "\n",
      "Step 2: Processing annotated file (filtering, k-mers)...\n",
      "\n",
      "Processing 08_07_24_GM12878_chr12-112000000-114000000...\n",
      "  Before filtering: 1,088,323 rows\n",
      "  After filtering: 1,024 rows\n",
      "  After deduplication: 172 rows\n",
      "Processing k-mers for 172 rows using 8 cores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing k-mers: 100%|██████████| 1/1 [00:00<00:00, 34.91chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Final dataset: 172 rows\n",
      "  Saved to: ../../Exemplar_Data/annotated_output/pickle_output/08_07_24_GM12878_chr12-112000000-114000000_annotated_valid_kmer.pkl\n",
      "\n",
      "Summary for 08_07_24_GM12878_chr12-112000000-114000000:\n",
      "  Total rows: 172\n",
      "  Unique genes: 17\n",
      "  Modification types:\n",
      "    a: 105\n",
      "    m: 44\n",
      "    17596: 14\n",
      "    19227: 4\n",
      "    17802: 2\n",
      "    69426: 1\n",
      "    19229: 1\n",
      "    19228: 1\n",
      "\n",
      "============================================================\n",
      "Pipeline Complete!\n",
      "Completed at: 2025-11-04 15:38:27\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RNA Modification Annotation and Processing Pipeline\")\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "for modkit_bed_file, sample_name in zip(MODKIT_FILES, SAMPLE_NAMES):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing sample: {sample_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Step 1: Annotate the modkit BED file with gene information\n",
    "    annotated_file = os.path.join(OUTPUT_DIR, f\"{sample_name}_annotated.bed\")\n",
    "    \n",
    "    if not os.path.exists(annotated_file):\n",
    "        print(\"\\nStep 1: Annotating modkit BED file with gene information...\")\n",
    "        annotate_bed_file(modkit_bed_file, trees, annotated_file, N_PROCESSES)\n",
    "    else:\n",
    "        print(f\"\\nStep 1: Using existing annotated file: {annotated_file}\")\n",
    "    \n",
    "    # Step 2: Process annotated file - add k-mers, filter, deduplicate\n",
    "    print(\"\\nStep 2: Processing annotated file (filtering, k-mers)...\")\n",
    "    result = process_annotated_file(\n",
    "        annotated_file,     # The annotated version\n",
    "        modkit_bed_file,    # The original modkit file (for additional columns)\n",
    "        FASTA_FILE, \n",
    "        sample_name, \n",
    "        PICKLE_DIR, \n",
    "        N_CORES\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nSummary for {sample_name}:\")\n",
    "    print(f\"  Total rows: {len(result):,}\")\n",
    "    print(f\"  Unique genes: {result['gene_id'].nunique():,}\")\n",
    "    print(f\"  Modification types:\")\n",
    "    for mod, count in result['mod'].value_counts().items():\n",
    "        print(f\"    {mod}: {count:,}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Pipeline Complete!\")\n",
    "print(f\"Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade279d-60fd-4075-9f7c-04f1a47b6bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
